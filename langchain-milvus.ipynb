{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764bfbe8-ecf9-49a9-9ec1-107d3a6b7fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"samsungreport.pdf\") #wget https://images.samsung.com/is/content/samsung/assets/global/ir/docs/2023_con_quarter04_all.pdf\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075422f4-c2c1-443e-b52b-240628c4e4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fe4e8f-6ac6-49a1-a730-9a4328863226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_embedding_model=\"all-MiniLM-L6-v2\" #git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "embeddings = HuggingFaceEmbeddings(model_name=local_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e610ccda-f8ad-41dd-b3ea-798a6fac7c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd8e94a-2db8-41c6-a9b8-719baa1ddbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_db = Milvus.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    collection_name=\"samsungreport2023\",\n",
    "    connection_args={\"host\": \"my-release-milvus.milvus.svc.cluster.local\", \"port\": \"19530\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92ac6e97-76d7-4fbb-8f11-d15538cea011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c44fbfe6-bc89-4c44-aa78-dd880187462d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default collection name - samsungreport2023\n",
      "Default search params - {'metric_type': 'L2', 'params': {'ef': 10}}\n",
      "Default index params - {'metric_type': 'L2', 'index_type': 'HNSW', 'params': {'M': 8, 'efConstruction': 64}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Default collection name - {vector_db.collection_name}\")\n",
    "print(f\"Default search params - {vector_db.search_params}\")\n",
    "print(f\"Default index params - {vector_db.index_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10e70268-1b60-4fcf-af1e-7f4aa225332e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Contents  \\n \\n \\n Page  \\n  \\nIndependent Auditors’ Report  1 \\n  \\nConsolidated Statements of Financial Position  4 \\n  \\nConsolidated Statements of Profit or Loss  7 \\n  \\nConsolidated Statements of Comprehensive Income  8 \\n  \\nConsolidated Statements of Changes in Equity  9 \\n  \\nConsolidated Statements of Cash Flows  13 \\n  \\nNotes to the Consolidated Financial Statements  15', metadata={'source': 'samsungreport.pdf', 'page': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d5b9d-067a-4783-813b-a299d4343b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Collection>:\n",
       "-------------\n",
       "<name>: samsungreport2023\n",
       "<description>: \n",
       "<schema>: {'auto_id': True, 'description': '', 'fields': [{'name': 'source', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'page', 'description': '', 'type': <DataType.INT64: 5>}, {'name': 'text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'pk', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'enable_dynamic_field': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9ab43b6-207c-43fd-8731-d66184bbf05f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 196 tensors from gpt4all-falcon-newbpe-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = falcon\n",
      "llama_model_loader: - kv   1:                               general.name str              = Falcon\n",
      "llama_model_loader: - kv   2:                      falcon.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                  falcon.tensor_data_layout str              = jploski\n",
      "llama_model_loader: - kv   4:                    falcon.embedding_length u32              = 4544\n",
      "llama_model_loader: - kv   5:                 falcon.feed_forward_length u32              = 18176\n",
      "llama_model_loader: - kv   6:                         falcon.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                falcon.attention.head_count u32              = 71\n",
      "llama_model_loader: - kv   8:             falcon.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   9:        falcon.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,65024]   = [\">>TITLE<<\", \">>ABSTRACT<<\", \">>INTR...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,65024]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,65024]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,64784]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"r e\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 11\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_0:  129 tensors\n",
      "llama_model_loader: - type q8_0:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 12/65024 vs 0/65024 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = falcon\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 65024\n",
      "llm_load_print_meta: n_merges         = 64784\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4544\n",
      "llm_load_print_meta: n_head           = 71\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 71\n",
      "llm_load_print_meta: n_embd_k_gqa     = 64\n",
      "llm_load_print_meta: n_embd_v_gqa     = 64\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 18176\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.22 B\n",
      "llm_load_print_meta: model size       = 3.92 GiB (4.66 BPW) \n",
      "llm_load_print_meta: general.name     = Falcon\n",
      "llm_load_print_meta: BOS token        = 11 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 11 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 138 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.08 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4013.48 MiB\n",
      "...................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   32.00 MiB, K (f16):   16.00 MiB, V (f16):   16.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.25 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     9.70 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1064\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'falcon.attention.head_count': '71', 'general.name': 'Falcon', 'falcon.context_length': '2048', 'general.architecture': 'falcon', 'falcon.tensor_data_layout': 'jploski', 'falcon.feed_forward_length': '18176', 'falcon.embedding_length': '4544', 'tokenizer.ggml.eos_token_id': '11', 'falcon.block_count': '32', 'falcon.attention.layer_norm_epsilon': '0.000010', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'falcon.attention.head_count_kv': '1', 'general.file_type': '2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "local_path = (\n",
    "    \"gpt4all-falcon-newbpe-q4_0.gguf\"  #wget https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf\n",
    ")\n",
    "llm = LlamaCpp(\n",
    "    model_path=local_path,\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    n_ctx=4096,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e436408-6360-48c2-b81c-545bb177e248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I am sorry, I do not have access to real-time financial data. However, according to Samsung's Q2 2023 earnings report, the company reported a total operating revenue of KRW 17.6 trillion (approximately $14.5 billion USD) for that period."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     577.02 ms\n",
      "llama_print_timings:      sample time =      27.53 ms /    64 runs   (    0.43 ms per token,  2324.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1215.46 ms /    18 tokens (   67.53 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:        eval time =    7692.15 ms /    63 runs   (  122.10 ms per token,     8.19 tokens per second)\n",
      "llama_print_timings:       total time =    9347.06 ms /    81 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Answer: I am sorry, I do not have access to real-time financial data. However, according to Samsung's Q2 2023 earnings report, the company reported a total operating revenue of KRW 17.6 trillion (approximately $14.5 billion USD) for that period.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Question: What is the actual Samsung's revenue in 2023?\n",
    "\"\"\"\n",
    "llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0228f9-ca9c-46c4-b168-b6148fbc61b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mContext information is below. \n",
      "------------\n",
      "Samsung Electronics Co., Ltd. and its subsidiaries  \n",
      " \n",
      "CONSOLIDATED STATEMENTS OF PROFIT OR LOSS  \n",
      " \n",
      " \n",
      "The above consolidated statement s of profit or loss  should be read in conjunction with the accompanying notes.  \n",
      "  \n",
      "- 7 - (In millions of Korean w on, in thousands of US dollars (Note  2.18)) \n",
      " \n",
      "    For the year s ended December 31,  \n",
      "  Notes  2023 2022 2023 2022 \n",
      "  KRW  KRW  USD  USD  \n",
      "      \n",
      "Revenue  29 258,935,494  302,231,360  198,247,859  231,396,319  \n",
      "Cost of sales  21 180,388,580  190,041,770  138,110,266  145,501,003  \n",
      "Gross profit   78,546,914  112,189,590  60,137,593  85,895,316  \n",
      "Selling and administrative expenses  21, 22  71,979,938  68,812,960  55,109,743  52,685,021  \n",
      "Operating profit  29 6,566,976  43,376,630  5,027,850  33,210,295  \n",
      "Other non -operating income  23 1,180,448  1,962,071  903,782  1,502,213  \n",
      "Other non-operating expense  23 1,083,327  1,790,176  829,424  1,370,606  \n",
      "Share of net profit of associates and joint ventures  9 887,550  1,090,643  679,532  835,025  \n",
      "Financial income  24 16,100,148  20,828,995  12,326,699  15,947,229  \n",
      "Financial expense  24 12,645,530  19,027,689  9,681,752  14,568,101  \n",
      "Profit before income tax   11,006,265  46,440,474  8,426,687  35,556,055  \n",
      "Income tax benefit  25 (4,480,835)  (9,213,603)  (3,430,646)  (7,054,178)  \n",
      "Profit for the year    15,487,100  55,654,077  11,857,333  42,610,233  \n",
      "Profit attributable to       \n",
      "Owners of the parent c ompany  14,473,401  54,730,018  11,081,218  41,902,749  \n",
      "Non-controlling interests    1,013,699  924,059  776,115  707,484  \n",
      "Earnings per share  \n",
      "(in Korean won, in US dollars)  26         \n",
      "- Basic   2,131  8,057  1.63 6.17 \n",
      "- Diluted    2,131  8,057  1.63 6.17\n",
      "------------\n",
      "Given the context information and not prior knowledge, answer the question: What is Samsung's revenue in 2023 in KRW?\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samsung's revenue in 2023 in KRW is 29,258,935,494."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     577.02 ms\n",
      "llama_print_timings:      sample time =      10.19 ms /    22 runs   (    0.46 ms per token,  2160.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3439.45 ms /    22 runs   (  156.34 ms per token,     6.40 tokens per second)\n",
      "llama_print_timings:       total time =    3626.21 ms /    23 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Samsung's revenue in 2023 in KRW is 29,258,935,494.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs={\n",
    "        \"refine_prompt\": QA_CHAIN_PROMPT\n",
    "        }\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1}),\n",
    "    return_source_documents=False,\n",
    "    callbacks=None,\n",
    "    chain_type_kwargs={\"refine_prompt\": QA_CHAIN_PROMPT,\"verbose\":True},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "question = \"What is Samsung's revenue in 2023 in KRW?\"\n",
    "result = qa_chain({\"query\": question}) # must be query\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0dbca-9474-4061-bb1a-35e3ebe8e38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
